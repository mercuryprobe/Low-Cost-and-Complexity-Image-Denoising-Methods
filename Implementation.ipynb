{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import init\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "from torch.utils import data\n",
        "from torchmetrics.image import PeakSignalNoiseRatio as PSNR\n",
        "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIM\n",
        "from torchvision.io import ImageReadMode, read_image\n",
        "from torchvision.transforms import Resize\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw1LCeql8kh6"
      },
      "source": [
        "code from NAFNet github link - https://github.com/megvii-research/NAFNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXE4NWObrgdy"
      },
      "outputs": [],
      "source": [
        "class LayerNormFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, weight, bias, eps):\n",
        "        ctx.eps = eps\n",
        "        N, C, H, W = x.size()\n",
        "        mu = x.mean(1, keepdim=True)\n",
        "        var = (x - mu).pow(2).mean(1, keepdim=True)\n",
        "        y = (x - mu) / (var + eps).sqrt()\n",
        "        ctx.save_for_backward(y, var, weight)\n",
        "        y = weight.view(1, C, 1, 1) * y + bias.view(1, C, 1, 1)\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        eps = ctx.eps\n",
        "\n",
        "        N, C, H, W = grad_output.size()\n",
        "        y, var, weight = ctx.saved_variables\n",
        "        g = grad_output * weight.view(1, C, 1, 1)\n",
        "        mean_g = g.mean(dim=1, keepdim=True)\n",
        "\n",
        "        mean_gy = (g * y).mean(dim=1, keepdim=True)\n",
        "        gx = 1.0 / torch.sqrt(var + eps) * (g - y * mean_gy - mean_g)\n",
        "        return (\n",
        "            gx,\n",
        "            (grad_output * y).sum(dim=3).sum(dim=2).sum(dim=0),\n",
        "            grad_output.sum(dim=3).sum(dim=2).sum(dim=0),\n",
        "            None,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LayerNorm2d(nn.Module):\n",
        "    def __init__(self, channels, eps=1e-6):\n",
        "        super(LayerNorm2d, self).__init__()\n",
        "        self.register_parameter(\"weight\", nn.Parameter(torch.ones(channels)))\n",
        "        self.register_parameter(\"bias\", nn.Parameter(torch.zeros(channels)))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        return LayerNormFunction.apply(x, self.weight, self.bias, self.eps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1-mv69Z0rSI"
      },
      "outputs": [],
      "source": [
        "class AvgPool2d(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size=None,\n",
        "        base_size=None,\n",
        "        auto_pad=True,\n",
        "        fast_imp=False,\n",
        "        train_size=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.base_size = base_size\n",
        "        self.auto_pad = auto_pad\n",
        "\n",
        "        # only used for fast implementation\n",
        "        self.fast_imp = fast_imp\n",
        "        self.rs = [5, 4, 3, 2, 1]\n",
        "        self.max_r1 = self.rs[0]\n",
        "        self.max_r2 = self.rs[0]\n",
        "        self.train_size = train_size\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return \"kernel_size={}, base_size={}, stride={}, fast_imp={}\".format(\n",
        "            self.kernel_size, self.base_size, self.kernel_size, self.fast_imp\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.kernel_size is None and self.base_size:\n",
        "            train_size = self.train_size\n",
        "            if isinstance(self.base_size, int):\n",
        "                self.base_size = (self.base_size, self.base_size)\n",
        "            self.kernel_size = list(self.base_size)\n",
        "            self.kernel_size[0] = x.shape[2] * self.base_size[0] // train_size[-2]\n",
        "            self.kernel_size[1] = x.shape[3] * self.base_size[1] // train_size[-1]\n",
        "\n",
        "            # only used for fast implementation\n",
        "            self.max_r1 = max(1, self.rs[0] * x.shape[2] // train_size[-2])\n",
        "            self.max_r2 = max(1, self.rs[0] * x.shape[3] // train_size[-1])\n",
        "\n",
        "        if self.kernel_size[0] >= x.size(-2) and self.kernel_size[1] >= x.size(-1):\n",
        "            return F.adaptive_avg_pool2d(x, 1)\n",
        "\n",
        "        if self.fast_imp:  # Non-equivalent implementation but faster\n",
        "            h, w = x.shape[2:]\n",
        "            if self.kernel_size[0] >= h and self.kernel_size[1] >= w:\n",
        "                out = F.adaptive_avg_pool2d(x, 1)\n",
        "            else:\n",
        "                r1 = [r for r in self.rs if h % r == 0][0]\n",
        "                r2 = [r for r in self.rs if w % r == 0][0]\n",
        "                # reduction_constraint\n",
        "                r1 = min(self.max_r1, r1)\n",
        "                r2 = min(self.max_r2, r2)\n",
        "                s = x[:, :, ::r1, ::r2].cumsum(dim=-1).cumsum(dim=-2)\n",
        "                n, c, h, w = s.shape\n",
        "                k1, k2 = min(h - 1, self.kernel_size[0] // r1), min(\n",
        "                    w - 1, self.kernel_size[1] // r2\n",
        "                )\n",
        "                out = (\n",
        "                    s[:, :, :-k1, :-k2]\n",
        "                    - s[:, :, :-k1, k2:]\n",
        "                    - s[:, :, k1:, :-k2]\n",
        "                    + s[:, :, k1:, k2:]\n",
        "                ) / (k1 * k2)\n",
        "                out = torch.nn.functional.interpolate(out, scale_factor=(r1, r2))\n",
        "        else:\n",
        "            n, c, h, w = x.shape\n",
        "            s = x.cumsum(dim=-1).cumsum_(dim=-2)\n",
        "            s = torch.nn.functional.pad(s, (1, 0, 1, 0))  # pad 0 for convenience\n",
        "            k1, k2 = min(h, self.kernel_size[0]), min(w, self.kernel_size[1])\n",
        "            s1, s2, s3, s4 = (\n",
        "                s[:, :, :-k1, :-k2],\n",
        "                s[:, :, :-k1, k2:],\n",
        "                s[:, :, k1:, :-k2],\n",
        "                s[:, :, k1:, k2:],\n",
        "            )\n",
        "            out = s4 + s1 - s2 - s3\n",
        "            out = out / (k1 * k2)\n",
        "\n",
        "        if self.auto_pad:\n",
        "            n, c, h, w = x.shape\n",
        "            _h, _w = out.shape[2:]\n",
        "            # print(x.shape, self.kernel_size)\n",
        "            pad2d = ((w - _w) // 2, (w - _w + 1) // 2, (h - _h) // 2, (h - _h + 1) // 2)\n",
        "            out = torch.nn.functional.pad(out, pad2d, mode=\"replicate\")\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def replace_layers(model, base_size, train_size, fast_imp, **kwargs):\n",
        "    for n, m in model.named_children():\n",
        "        if len(list(m.children())) > 0:\n",
        "            ## compound module, go inside it\n",
        "            replace_layers(m, base_size, train_size, fast_imp, **kwargs)\n",
        "\n",
        "        if isinstance(m, nn.AdaptiveAvgPool2d):\n",
        "            pool = AvgPool2d(\n",
        "                base_size=base_size, fast_imp=fast_imp, train_size=train_size\n",
        "            )\n",
        "            assert m.output_size == 1\n",
        "            setattr(model, n, pool)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "ref.\n",
        "@article{chu2021tlsc,\n",
        "  title={Revisiting Global Statistics Aggregation for Improving Image Restoration},\n",
        "  author={Chu, Xiaojie and Chen, Liangyu and and Chen, Chengpeng and Lu, Xin},\n",
        "  journal={arXiv preprint arXiv:2112.04491},\n",
        "  year={2021}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Local_Base:\n",
        "    def convert(self, *args, train_size, **kwargs):\n",
        "        replace_layers(self, *args, train_size=train_size, **kwargs)\n",
        "        imgs = torch.rand(train_size)\n",
        "        with torch.no_grad():\n",
        "            self.forward(imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.randint(0, 255, size=4)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw_Bwi9-1btL"
      },
      "outputs": [],
      "source": [
        "class NoiseDataset(data.Dataset):\n",
        "    # dataset with noise 2 noise augmentation\n",
        "    # noise is generated by random seed\n",
        "\n",
        "    def __init__(self, image_dir, size=512, num_create=4):\n",
        "        self.images_dir = image_dir\n",
        "        self.images = os.listdir(image_dir)\n",
        "\n",
        "        self.size = size\n",
        "        self.resize = Resize((size, size))\n",
        "        self.num_create = num_create\n",
        "        self.sigma = 0.1\n",
        "\n",
        "        self.data = {}\n",
        "        for i in range(len(self.images)):\n",
        "            self.data[self.images[i]] = np.random.randint(0, 255, size=self.num_create)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images) * (self.num_create * self.num_create)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clean_idx = idx // (self.num_create * self.num_create)\n",
        "        sub_idx = idx % (self.num_create * self.num_create)\n",
        "\n",
        "        clean_image_dir = self.images[clean_idx]\n",
        "\n",
        "        src_idx = sub_idx % self.num_create\n",
        "        tgt_idx = sub_idx // self.num_create\n",
        "\n",
        "        src_generator = torch.Generator().manual_seed(\n",
        "            int(self.data[clean_image_dir][src_idx])\n",
        "        )\n",
        "        tgt_generator = torch.Generator().manual_seed(\n",
        "            int(self.data[clean_image_dir][tgt_idx])\n",
        "        )\n",
        "\n",
        "        # load img\n",
        "        src_image = (\n",
        "            self.resize(\n",
        "                read_image(f\"{self.images_dir}/{clean_image_dir}\", ImageReadMode.RGB)\n",
        "            )\n",
        "            / 255.0\n",
        "        )\n",
        "        tgt_image = (\n",
        "            self.resize(\n",
        "                read_image(f\"{self.images_dir}/{clean_image_dir}\", ImageReadMode.RGB)\n",
        "            )\n",
        "            / 255.0\n",
        "        )\n",
        "\n",
        "        # add noise\n",
        "        src_image += (\n",
        "            torch.randn(size=(self.size, self.size), generator=src_generator)\n",
        "            * self.sigma\n",
        "        )\n",
        "        tgt_image += (\n",
        "            torch.randn(size=(self.size, self.size), generator=tgt_generator)\n",
        "            * self.sigma\n",
        "            * (tgt_idx != src_idx)\n",
        "        )\n",
        "\n",
        "        # clip\n",
        "        src_image = torch.clip(src_image, 0, 1)  # (3, size, size)\n",
        "        tgt_image = torch.clip(tgt_image, 0, 1)  # (3, size, size)\n",
        "\n",
        "        return src_image, tgt_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cu8jPOIRgl0K"
      },
      "outputs": [],
      "source": [
        "class SimpleGate(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x1, x2 = x.chunk(2, dim=1)\n",
        "        return x1 * x2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NAFBlock(nn.Module):\n",
        "    def __init__(self, c, DW_Expand=2, FFN_Expand=2, drop_out_rate=0.0):\n",
        "        super().__init__()\n",
        "        dw_channel = c * DW_Expand\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=c,\n",
        "            out_channels=dw_channel,\n",
        "            kernel_size=1,\n",
        "            padding=0,\n",
        "            stride=1,\n",
        "            groups=1,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=dw_channel,\n",
        "            out_channels=dw_channel,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            stride=1,\n",
        "            groups=dw_channel,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            in_channels=dw_channel // 2,\n",
        "            out_channels=c,\n",
        "            kernel_size=1,\n",
        "            padding=0,\n",
        "            stride=1,\n",
        "            groups=1,\n",
        "            bias=True,\n",
        "        )\n",
        "\n",
        "        # Simplified Channel Attention\n",
        "        self.sca = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(\n",
        "                in_channels=dw_channel // 2,\n",
        "                out_channels=dw_channel // 2,\n",
        "                kernel_size=1,\n",
        "                padding=0,\n",
        "                stride=1,\n",
        "                groups=1,\n",
        "                bias=True,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # SimpleGate\n",
        "        self.sg = SimpleGate()\n",
        "\n",
        "        ffn_channel = FFN_Expand * c\n",
        "        self.conv4 = nn.Conv2d(\n",
        "            in_channels=c,\n",
        "            out_channels=ffn_channel,\n",
        "            kernel_size=1,\n",
        "            padding=0,\n",
        "            stride=1,\n",
        "            groups=1,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.conv5 = nn.Conv2d(\n",
        "            in_channels=ffn_channel // 2,\n",
        "            out_channels=c,\n",
        "            kernel_size=1,\n",
        "            padding=0,\n",
        "            stride=1,\n",
        "            groups=1,\n",
        "            bias=True,\n",
        "        )\n",
        "\n",
        "        self.norm1 = LayerNorm2d(c)\n",
        "        self.norm2 = LayerNorm2d(c)\n",
        "\n",
        "        self.dropout1 = (\n",
        "            nn.Dropout(drop_out_rate) if drop_out_rate > 0.0 else nn.Identity()\n",
        "        )\n",
        "        self.dropout2 = (\n",
        "            nn.Dropout(drop_out_rate) if drop_out_rate > 0.0 else nn.Identity()\n",
        "        )\n",
        "\n",
        "        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
        "        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = inp\n",
        "\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.sg(x)\n",
        "        x = x * self.sca(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        y = inp + x * self.beta\n",
        "\n",
        "        x = self.conv4(self.norm2(y))\n",
        "        x = self.sg(x)\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        return y + x * self.gamma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NAFNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_channel=3,\n",
        "        width=16,\n",
        "        middle_blk_num=1,\n",
        "        enc_blk_nums=[],\n",
        "        dec_blk_nums=[],\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.intro = nn.Conv2d(\n",
        "            in_channels=img_channel,\n",
        "            out_channels=width,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            stride=1,\n",
        "            groups=1,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.ending = nn.Conv2d(\n",
        "            in_channels=width,\n",
        "            out_channels=img_channel,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            stride=1,\n",
        "            groups=1,\n",
        "            bias=True,\n",
        "        )\n",
        "\n",
        "        self.encoders = nn.ModuleList()\n",
        "        self.decoders = nn.ModuleList()\n",
        "        self.middle_blks = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "\n",
        "        chan = width\n",
        "        for num in enc_blk_nums:\n",
        "            self.encoders.append(nn.Sequential(*[NAFBlock(chan) for _ in range(num)]))\n",
        "            self.downs.append(nn.Conv2d(chan, 2 * chan, 2, 2))\n",
        "            chan = chan * 2\n",
        "\n",
        "        self.middle_blks = nn.Sequential(\n",
        "            *[NAFBlock(chan) for _ in range(middle_blk_num)]\n",
        "        )\n",
        "\n",
        "        for num in dec_blk_nums:\n",
        "            self.ups.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(chan, chan * 2, 1, bias=False), nn.PixelShuffle(2)\n",
        "                )\n",
        "            )\n",
        "            chan = chan // 2\n",
        "            self.decoders.append(nn.Sequential(*[NAFBlock(chan) for _ in range(num)]))\n",
        "\n",
        "        self.padder_size = 2 ** len(self.encoders)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        B, C, H, W = inp.shape\n",
        "        inp = self.check_image_size(inp)\n",
        "\n",
        "        x = self.intro(inp)\n",
        "\n",
        "        encs = []\n",
        "\n",
        "        for encoder, down in zip(self.encoders, self.downs):\n",
        "            x = encoder(x)\n",
        "            encs.append(x)\n",
        "            x = down(x)\n",
        "\n",
        "        x = self.middle_blks(x)\n",
        "\n",
        "        for decoder, up, enc_skip in zip(self.decoders, self.ups, encs[::-1]):\n",
        "            x = up(x)\n",
        "            x = x + enc_skip\n",
        "            x = decoder(x)\n",
        "\n",
        "        x = self.ending(x)\n",
        "        x = x + inp\n",
        "\n",
        "        return x[:, :, :H, :W]\n",
        "\n",
        "    def check_image_size(self, x):\n",
        "        _, _, h, w = x.size()\n",
        "        mod_pad_h = (self.padder_size - h % self.padder_size) % self.padder_size\n",
        "        mod_pad_w = (self.padder_size - w % self.padder_size) % self.padder_size\n",
        "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NAFNetLocal(Local_Base, NAFNet):\n",
        "    def __init__(self, *args, train_size=(1, 3, 256, 256), fast_imp=False, **kwargs):\n",
        "        Local_Base.__init__(self)\n",
        "        NAFNet.__init__(self, *args, **kwargs)\n",
        "\n",
        "        N, C, H, W = train_size\n",
        "        base_size = (int(H * 1.5), int(W * 1.5))\n",
        "\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.convert(base_size=base_size, train_size=train_size, fast_imp=fast_imp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_loop(dataloader, model, criterion, is_PSNR=False):\n",
        "    model.eval()\n",
        "\n",
        "    if not is_PSNR:\n",
        "        psnr = PSNR().to(device)\n",
        "\n",
        "    total_psnr = 0.0\n",
        "    total_loss = 0.0\n",
        "    for inputs, targets in tqdm(dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        pred = model(inputs)\n",
        "        if not is_PSNR:\n",
        "            total_psnr += psnr(pred, targets).item()\n",
        "        total_loss += criterion(pred, targets).item()\n",
        "\n",
        "    return total_psnr / len(dataloader.dataset), total_loss / len(dataloader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Dataset\n",
        "\n",
        "dataset = NoiseDataset('images/bsd_all')\n",
        "train_dataset, val_dataset, test_dataset = data.random_split(dataset, [0.6, 0.2, 0.2], generator=torch.Generator().manual_seed(42))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Dataloader\n",
        "\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = data.DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "model = NAFNet()\n",
        "model = model.to(device)\n",
        "optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss().to(device)\n",
        "criterion = PSNR(data_range=1.0).to(device)\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def train(model, train_loader, criterion, optimiser, num_epochs, is_PSNR=False):\n",
        "    model.to(device)\n",
        "\n",
        "    psnr_history_train = []\n",
        "    psnr_history_val = []\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        cur_loss = 0.0\n",
        "\n",
        "        for inputs, targets in tqdm(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            optimiser.zero_grad()\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "            cur_loss += loss.item()\n",
        "\n",
        "        epoch_loss = cur_loss / len(train_loader.dataset)\n",
        "        overall_psnr_train, _ = eval_loop(train_loader, model, criterion, is_PSNR)\n",
        "        overall_psnr_val, _ = eval_loop(val_loader, model, criterion, is_PSNR)\n",
        "\n",
        "        loss_history.append(epoch_loss)\n",
        "        psnr_history_train.append(overall_psnr_train)\n",
        "        psnr_history_val.append(overall_psnr_val)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "        print(f\"PSNR (train): {overall_psnr_train:.4f}, PSNR (val): {overall_psnr_val:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwl78QRw1hbM"
      },
      "outputs": [],
      "source": [
        "loss_history, psnr_history_train, psnr_history_val = train(model, train_loader, criterion, optimiser, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_psnr, test_loss = eval_loop(test_loader, model, criterion)\n",
        "print(test_psnr, test_loss)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
